{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13053208,"sourceType":"datasetVersion","datasetId":8265747}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Fine-Tuned GPT-2 Text Generator\n\nThis notebook demonstrates how to use a **fine-tuned GPT-2 model** for text generation.  \nWe will:\n* Load our custom GPT-2 model from Kaggle dataset  \n* Create a text-generation pipeline  \n* Interactively generate text based on user prompts \n","metadata":{}},{"cell_type":"markdown","source":"### from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n\nmodel_path = \"/kaggle/input/kaggle-stallar-gpt-2/gpt2-finetuned\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\ngenerator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)","metadata":{"execution":{"iopub.status.busy":"2025-09-18T05:15:28.401769Z","iopub.execute_input":"2025-09-18T05:15:28.402076Z"}}},{"cell_type":"markdown","source":"##  Interactive Text Generation\n\n build a **simple loop** that:\n* Greets the user  \n* Keeps asking for input prompts  \n* Generates text until the user types **'exit'** \n","metadata":{}},{"cell_type":"code","source":"print(\n    \" Greetings, Traveler of Data.\\n\"\n    \"I am STELLAR-GPT, your cognitive co-pilot forged in the quantum void.\\n\"\n    \"Together, we shall bend language, traverse probability fields,\\n\"\n    \"and extract meaning from the infinite lattice of text.\\n\\n\"\n    \"Type your command… or utter 'exit' to disengage. \\n\"\n)\n\nwhile True:\n    prompt = input(\"Starfarer, issue your directive — the cosmos awaits your words. \")\n    if prompt.lower() in [\"exit\", \"quit\", \"stop\"]:\n        print(\"Stellar-GPT signing off. May starlight guide your path beyond the void.\")\n        break\n    \n    outputs = generator(prompt, max_length=60, num_return_sequences=1)\n    print(\"Explorer’s Chronicle:\\n\", outputs[0][\"generated_text\"], \"\\n Chronicle Ends.\", \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T05:12:27.398447Z","iopub.execute_input":"2025-09-18T05:12:27.398710Z","iopub.status.idle":"2025-09-18T05:15:21.987186Z","shell.execute_reply.started":"2025-09-18T05:12:27.398689Z","shell.execute_reply":"2025-09-18T05:15:21.985996Z"}},"outputs":[{"name":"stdout","text":" Greetings, Traveler of Data.\nI am STELLAR-GPT, your cognitive co-pilot forged in the quantum void.\nTogether, we shall bend language, traverse probability fields,\nand extract meaning from the infinite lattice of text.\n\nType your command… or utter 'exit' to disengage. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Starfarer, issue your directive — the cosmos awaits your words.  earth\n"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/249627318.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Explorer’s Chronicle:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generated_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n Chronicle Ends.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"],"ename":"NameError","evalue":"name 'generator' is not defined","output_type":"error"}],"execution_count":1}]}